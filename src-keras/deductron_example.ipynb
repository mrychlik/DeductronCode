{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the deductron layer\n",
    "\n",
    "This notebook implements one of the examples from the Keras documentation for sequence-to-sequence learning using RNNs. The task is to solve addition problems, formulated as strings. For example, the network should be able to take the string `341+78 ` and return the sum `419`.\n",
    "\n",
    "This is accomplished using an encoding layer and a decoding layer. In the documentation example, both the encoder and the decoder are LSTMs. In this notebook, we generate some training data and train networks using both LSTMs and deductron layers.\n",
    "\n",
    "The code for this example is taken from https://keras.io/examples/nlp/addition_rnn/ and lightly modified to fit our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import deductron\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True        # Reverse the order of the input strings -- this seems to improve learning\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# Although a sigmoid is a natural choice for an activation with values in (0, 1), a ReLU\n",
    "# constrained to produce output in [0, 1] has much better performance.\n",
    "clipped_relu = lambda x: keras.activations.relu(x, max_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate addition problems for training\n",
    "\n",
    "class CharacterTable:\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot or integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "            C: string, to be encoded.\n",
    "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
    "                or a vector of character indices (used with `calc_argmax=False`).\n",
    "            calc_argmax: Whether to find the character index with maximum\n",
    "                probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = \"0123456789+ \"\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(\n",
    "        \"\".join(\n",
    "            np.random.choice(list(\"0123456789\"))\n",
    "            for i in range(np.random.randint(1, DIGITS + 1))\n",
    "        )\n",
    "    )\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = \"{}+{}\".format(a, b)\n",
    "    query = q + \" \" * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the training and validation data\n",
    "\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference example: LSTM encoder/decoder\n",
    "\n",
    "The purpose of the next few sections is to compare the LSTM solution with a pure deductron solution as well as mixed LSTM-deductron solutions.\n",
    "\n",
    "The reference approach uses two layers, thinking of the first as an encoder and the second as a decoder. The first layer extracts features from the input and produces an internal representation of the problem. The final output of this \"encoder\" layer is then repeated to form a constant sequence and passed to the \"decoder\" layer as its input.\n",
    "\n",
    "Following the existing example, we begin by building a model in which each of the encoder and decoder is a single LSTM layer with 128 hidden units. Further examples will replace each of these layers in turn with a deductron layer (also with 128 units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4, 12)             1548      \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# \"Encode\" the input sequence using a LSTM, producing an output of size 128.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "# Replacing the encoder with a deductron layer seems to hurt accuracy.\n",
    "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last output of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# We'll use a single layer for the decoder -- adding more layers doesn't seem to improve results\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the model. We will train all of the models for 12 epochs (more would be better, but the training is a little time consuming). After each epoch we visualize 10 samples from the validation set so that we can see the sort of output we are getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 378us/step - loss: 1.7643 - accuracy: 0.3549 - val_loss: 1.5724 - val_accuracy: 0.4114\n",
      "Q 450+30  T 480  ☒ 555 \n",
      "Q 53+854  T 907  ☒ 603 \n",
      "Q 601+43  T 644  ☒ 657 \n",
      "Q 280+21  T 301  ☒ 235 \n",
      "Q 619+996 T 1615 ☒ 1633\n",
      "Q 891+321 T 1212 ☒ 1035\n",
      "Q 43+545  T 588  ☒ 555 \n",
      "Q 249+158 T 407  ☒ 587 \n",
      "Q 63+639  T 702  ☒ 687 \n",
      "Q 823+491 T 1314 ☒ 1135\n",
      "\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 359us/step - loss: 1.3714 - accuracy: 0.4862 - val_loss: 1.1968 - val_accuracy: 0.5591\n",
      "Q 86+76   T 162  ☒ 155 \n",
      "Q 40+292  T 332  ☒ 319 \n",
      "Q 43+16   T 59   ☒ 40  \n",
      "Q 760+2   T 762  ☒ 771 \n",
      "Q 608+56  T 664  ☒ 689 \n",
      "Q 653+984 T 1637 ☒ 1509\n",
      "Q 486+409 T 895  ☒ 809 \n",
      "Q 33+39   T 72   ☒ 40  \n",
      "Q 750+73  T 823  ☒ 814 \n",
      "Q 998+90  T 1088 ☒ 1023\n",
      "\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 371us/step - loss: 1.0869 - accuracy: 0.5954 - val_loss: 0.9980 - val_accuracy: 0.6288\n",
      "Q 3+638   T 641  ☒ 647 \n",
      "Q 946+60  T 1006 ☒ 1002\n",
      "Q 614+456 T 1070 ☒ 1042\n",
      "Q 644+2   T 646  ☒ 647 \n",
      "Q 12+63   T 75   ☒ 71  \n",
      "Q 864+10  T 874  ☒ 875 \n",
      "Q 84+324  T 408  ☒ 492 \n",
      "Q 758+8   T 766  ☒ 772 \n",
      "Q 7+767   T 774  ☒ 772 \n",
      "Q 99+168  T 267  ☑ 267 \n",
      "\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 365us/step - loss: 0.9331 - accuracy: 0.6540 - val_loss: 0.8810 - val_accuracy: 0.6775\n",
      "Q 353+5   T 358  ☒ 357 \n",
      "Q 701+5   T 706  ☒ 700 \n",
      "Q 841+39  T 880  ☒ 879 \n",
      "Q 35+519  T 554  ☒ 569 \n",
      "Q 85+287  T 372  ☒ 351 \n",
      "Q 193+51  T 244  ☒ 247 \n",
      "Q 28+83   T 111  ☑ 111 \n",
      "Q 774+2   T 776  ☒ 779 \n",
      "Q 311+49  T 360  ☒ 359 \n",
      "Q 52+76   T 128  ☒ 121 \n",
      "\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 366us/step - loss: 0.8300 - accuracy: 0.6936 - val_loss: 0.7842 - val_accuracy: 0.7114\n",
      "Q 41+620  T 661  ☒ 668 \n",
      "Q 797+8   T 805  ☒ 800 \n",
      "Q 262+406 T 668  ☒ 671 \n",
      "Q 908+270 T 1178 ☒ 1189\n",
      "Q 11+999  T 1010 ☒ 1000\n",
      "Q 73+479  T 552  ☒ 557 \n",
      "Q 44+83   T 127  ☒ 123 \n",
      "Q 31+326  T 357  ☒ 356 \n",
      "Q 97+40   T 137  ☒ 133 \n",
      "Q 856+399 T 1255 ☒ 1253\n",
      "\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 365us/step - loss: 0.7409 - accuracy: 0.7276 - val_loss: 0.7154 - val_accuracy: 0.7400\n",
      "Q 910+11  T 921  ☒ 926 \n",
      "Q 94+718  T 812  ☒ 810 \n",
      "Q 725+6   T 731  ☑ 731 \n",
      "Q 2+621   T 623  ☒ 622 \n",
      "Q 59+76   T 135  ☒ 134 \n",
      "Q 789+161 T 950  ☒ 954 \n",
      "Q 213+256 T 469  ☒ 472 \n",
      "Q 568+80  T 648  ☒ 640 \n",
      "Q 443+24  T 467  ☒ 465 \n",
      "Q 72+305  T 377  ☒ 376 \n",
      "\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 364us/step - loss: 0.6475 - accuracy: 0.7643 - val_loss: 0.5925 - val_accuracy: 0.7812\n",
      "Q 75+741  T 816  ☒ 814 \n",
      "Q 704+699 T 1403 ☒ 1307\n",
      "Q 61+88   T 149  ☒ 159 \n",
      "Q 42+965  T 1007 ☒ 100 \n",
      "Q 871+89  T 960  ☒ 968 \n",
      "Q 368+57  T 425  ☒ 424 \n",
      "Q 45+86   T 131  ☒ 139 \n",
      "Q 376+28  T 404  ☑ 404 \n",
      "Q 973+2   T 975  ☑ 975 \n",
      "Q 4+840   T 844  ☑ 844 \n",
      "\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 357us/step - loss: 0.4574 - accuracy: 0.8375 - val_loss: 0.3271 - val_accuracy: 0.8958\n",
      "Q 178+27  T 205  ☑ 205 \n",
      "Q 71+896  T 967  ☑ 967 \n",
      "Q 0+574   T 574  ☑ 574 \n",
      "Q 302+61  T 363  ☑ 363 \n",
      "Q 66+84   T 150  ☑ 150 \n",
      "Q 98+964  T 1062 ☑ 1062\n",
      "Q 855+614 T 1469 ☒ 1479\n",
      "Q 797+8   T 805  ☑ 805 \n",
      "Q 855+77  T 932  ☑ 932 \n",
      "Q 28+461  T 489  ☑ 489 \n",
      "\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 361us/step - loss: 0.2612 - accuracy: 0.9232 - val_loss: 0.2065 - val_accuracy: 0.9412\n",
      "Q 571+53  T 624  ☑ 624 \n",
      "Q 763+499 T 1262 ☑ 1262\n",
      "Q 14+18   T 32   ☑ 32  \n",
      "Q 564+49  T 613  ☑ 613 \n",
      "Q 580+9   T 589  ☑ 589 \n",
      "Q 77+145  T 222  ☑ 222 \n",
      "Q 530+8   T 538  ☑ 538 \n",
      "Q 611+875 T 1486 ☒ 1487\n",
      "Q 401+778 T 1179 ☑ 1179\n",
      "Q 797+8   T 805  ☑ 805 \n",
      "\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 370us/step - loss: 0.1563 - accuracy: 0.9619 - val_loss: 0.1268 - val_accuracy: 0.9655\n",
      "Q 706+737 T 1443 ☑ 1443\n",
      "Q 95+841  T 936  ☑ 936 \n",
      "Q 178+990 T 1168 ☑ 1168\n",
      "Q 642+345 T 987  ☑ 987 \n",
      "Q 42+860  T 902  ☒ 903 \n",
      "Q 767+8   T 775  ☑ 775 \n",
      "Q 56+48   T 104  ☑ 104 \n",
      "Q 12+999  T 1011 ☑ 1011\n",
      "Q 152+981 T 1133 ☒ 1134\n",
      "Q 896+723 T 1619 ☑ 1619\n",
      "\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 17s 368us/step - loss: 0.1052 - accuracy: 0.9755 - val_loss: 0.0745 - val_accuracy: 0.9848\n",
      "Q 52+982  T 1034 ☑ 1034\n",
      "Q 5+23    T 28   ☒ 38  \n",
      "Q 173+20  T 193  ☑ 193 \n",
      "Q 203+18  T 221  ☑ 221 \n",
      "Q 373+87  T 460  ☑ 460 \n",
      "Q 795+60  T 855  ☑ 855 \n",
      "Q 89+66   T 155  ☑ 155 \n",
      "Q 111+99  T 210  ☑ 210 \n",
      "Q 789+435 T 1224 ☑ 1224\n",
      "Q 70+594  T 664  ☑ 664 \n",
      "\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 16s 366us/step - loss: 0.0639 - accuracy: 0.9862 - val_loss: 0.0603 - val_accuracy: 0.9844\n",
      "Q 639+663 T 1302 ☑ 1302\n",
      "Q 912+43  T 955  ☑ 955 \n",
      "Q 270+999 T 1269 ☑ 1269\n",
      "Q 899+942 T 1841 ☑ 1841\n",
      "Q 216+75  T 291  ☑ 291 \n",
      "Q 888+761 T 1649 ☑ 1649\n",
      "Q 1+442   T 443  ☑ 443 \n",
      "Q 5+577   T 582  ☑ 582 \n",
      "Q 55+98   T 153  ☑ 153 \n",
      "Q 75+921  T 996  ☑ 996 \n"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for epoch in range(0, epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, after 12 epochs, the above network should reach about 98-99% accuracy, with most visualized problems solved correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A pure deductron implementation\n",
    "\n",
    "Our first change is to replace both the encoder and decoder LSTM layers with deductron layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3f5eb204dba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeductron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeductron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclipped_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAXLEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRepeatVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIGITS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeductron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeductron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclipped_relu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(deductron.Deductron(256, activation = clipped_relu, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "model.add(deductron.Deductron(256, activation = clipped_relu, return_sequences=True))\n",
    "\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      " 8864/45000 [====>.........................] - ETA: 10s - loss: 2.0272 - accuracy: 0.2970"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pure deductron network does improve accuracy with training, but learns much more slowly, reaching only around 58-60% accuracy after 12 epohcs. Visual inspection of the output shows many repeated digits and similar apparent patterns, possibly pointing to insufficient feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM encoder, deductron decoder\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "model.add(deductron.Deductron(128, activation = clipped_relu, return_sequences=True))\n",
    "\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy with the LSTM encoder and deductron decoder is comparable to using LSTM in both roles. This adds credibility to the idea that the problem in the previous test was insufficient feature extraction by the deductron layer.\n",
    "\n",
    "Reversing the roles of the LSTM and deductron adds more weight to this idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deductron encoder, LSTM decoder\n",
    "num_layers = 1  # Number of layers in the decoder.\n",
    "model = keras.Sequential()\n",
    "model.add(deductron.Deductron(128, activation = clipped_relu, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    print(\"Iteration\", epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=1,\n",
    "        validation_data=(x_val, y_val),\n",
    "    )\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
    "        print(\"T\", correct, end=\" \")\n",
    "        if correct == guess:\n",
    "            print(\"☑ \" + guess)\n",
    "        else:\n",
    "            print(\"☒ \" + guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network has only marginally better performance than the pure deductron, suggesting that the low performance was tied specifically to the deductron in the encoder role, not the absence of an LSTM layer in the pure deductron network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The above tests show that the deductron architecture is capable of learning the sequence-to-sequence addition task. However, in the encoder-decoder network structure, the deductron does not perform as well as LSTM in the \"encoder\" role. Using LSTM for the encoder and deductron for the decoder reaches approximate parity with the LSTM-LSTM network; reversing the roles leads to slower learning. This suggests that the deductron may not have enough complexity to extract the necessary features for this task, but is effective at using these features once learned by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
